{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPvVVTmwKqQr/OY8vHgJZsf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tobiolad/Working-with-PythonSQL/blob/main/LinkedIn_Extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lSKKbpAp6G0",
        "outputId": "4cbf743b-54f0-4e6a-f188-f7791b123ba8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Collecting googlesearch-python\n",
            "  Downloading googlesearch-python-1.2.3.tar.gz (3.9 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from googlesearch-python) (2.31.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->googlesearch-python) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->googlesearch-python) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->googlesearch-python) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->googlesearch-python) (2024.2.2)\n",
            "Building wheels for collected packages: googlesearch-python\n",
            "  Building wheel for googlesearch-python (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googlesearch-python: filename=googlesearch_python-1.2.3-py3-none-any.whl size=4209 sha256=15794551d40d59f81a95efdf44c6de03c228fa989dd7ae733216f0fc8ed1f46c\n",
            "  Stored in directory: /root/.cache/pip/wheels/98/24/e9/6c225502948c629b01cc895f86406819281ef0da385f3eb669\n",
            "Successfully built googlesearch-python\n",
            "Installing collected packages: googlesearch-python\n",
            "Successfully installed googlesearch-python-1.2.3\n"
          ]
        }
      ],
      "source": [
        "!pip install google googlesearch-python beautifulsoup4"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install selenium beautifulsoup4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TV4YJYi0qPrG",
        "outputId": "f5b8fb1b-ff7c-4398-8f98-b9f386541db6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting selenium\n",
            "  Downloading selenium-4.18.1-py3-none-any.whl (10.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: urllib3[socks]<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from selenium) (2.0.7)\n",
            "Collecting trio~=0.17 (from selenium)\n",
            "  Downloading trio-0.24.0-py3-none-any.whl (460 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m460.2/460.2 kB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting trio-websocket~=0.9 (from selenium)\n",
            "  Downloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (2024.2.2)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from selenium) (4.9.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n",
            "Requirement already satisfied: attrs>=20.1.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (23.2.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (3.6)\n",
            "Collecting outcome (from trio~=0.17->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.2.0)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket~=0.9->selenium)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: outcome, h11, wsproto, trio, trio-websocket, selenium\n",
            "Successfully installed h11-0.14.0 outcome-1.3.0.post0 selenium-4.18.1 trio-0.24.0 trio-websocket-0.11.1 wsproto-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import re\n",
        "import csv\n",
        "import os\n",
        "from time import sleep\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from googlesearch import search\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Define the sanitize_filename function\n",
        "def sanitize_filename(filename):\n",
        "    # Replace invalid characters with underscores\n",
        "    return re.sub(r'[<>:\"/\\\\|?*]', '_', filename)\n",
        "\n",
        "class LinkedIn:\n",
        "    def __init__(self):\n",
        "        self.fieldnames = [\"Profile Link\", \"Name\", \"Designation\", \"Location\"]\n",
        "\n",
        "    def saveData(self, dataset, output_file_name):\n",
        "        with open(output_file_name, mode='a+', encoding='utf-8-sig', newline='') as csvFile:\n",
        "            writer = csv.DictWriter(\n",
        "                csvFile, fieldnames=self.fieldnames, delimiter=',', quotechar='\"')\n",
        "            if os.stat(output_file_name).st_size == 0:\n",
        "                writer.writeheader()\n",
        "            writer.writerow({\n",
        "                \"Profile Link\": dataset[0],\n",
        "                \"Name\": dataset[1],\n",
        "                \"Designation\": dataset[2],\n",
        "                \"Location\": dataset[3]\n",
        "            })\n",
        "\n",
        "    @classmethod\n",
        "    def getCompanyID(cls, company_link):\n",
        "        headers = {\n",
        "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
        "            'Accept-Encoding': 'gzip, deflate, br',\n",
        "            'Accept-Language': 'en-US,en;q=0.9',\n",
        "            'Cache-Control': 'max-age=0',\n",
        "            'Dnt': '1',\n",
        "            'Sec-Ch-Ua': '\"Not A(Brand\";v=\"99\", \"Google Chrome\";v=\"121\", \"Chromium\";v=\"121\"',\n",
        "            'Sec-Ch-Ua-Mobile': '?0',\n",
        "            'Sec-Ch-Ua-Platform': '\"Windows\"',\n",
        "            'Sec-Fetch-Dest': 'document',\n",
        "            'Sec-Fetch-Mode': 'navigate',\n",
        "            'Sec-Fetch-Site': 'none',\n",
        "            'Sec-Fetch-User': '?1',\n",
        "            'Upgrade-Insecure-Requests': '1',\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',\n",
        "        }\n",
        "        try:\n",
        "            resp = requests.get(company_link, headers=headers).text\n",
        "        except:\n",
        "            print(\"Failed to open {}\".format(company_link))\n",
        "            return None\n",
        "        try:\n",
        "            companyID = re.findall(\n",
        "                r'\"objectUrn\":\"urn:li:organization:([\\d]+)\"', resp)[0]\n",
        "        except:\n",
        "            print(\"Company ID not found\")\n",
        "            return None\n",
        "        return companyID\n",
        "\n",
        "    def paginateResults(self, companyID, cookies, pagination_delay, output_file_name):\n",
        "        csrf_tokens = re.findall(r'JSESSIONID=\"(.+?)\"', cookies)\n",
        "        csrf_token = csrf_tokens[0] if csrf_tokens else ''\n",
        "        headers = {\n",
        "            'Accept': 'application/vnd.linkedin.normalized+json+2.1',\n",
        "            'Cookie': cookies,\n",
        "            'Csrf-Token': csrf_token,\n",
        "            'Dnt': '1',\n",
        "            'Referer': 'https://www.linkedin.com/search/results/people/?currentCompany=%5B%22' + companyID + '%22%5D&origin=COMPANY_PAGE_CANNED_SEARCH&page=2&sid=7Gd',\n",
        "            'Sec-Ch-Ua': '\"Not A(Brand\";v=\"99\", \"Google Chrome\";v=\"121\", \"Chromium\";v=\"121\"',\n",
        "            'Sec-Ch-Ua-Mobile': '?0',\n",
        "            'Sec-Ch-Ua-Platform': '\"Windows\"',\n",
        "            'Sec-Fetch-Dest': 'empty',\n",
        "            'Sec-Fetch-Mode': 'cors',\n",
        "            'Sec-Fetch-Site': 'same-origin',\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',\n",
        "            'X-Li-Lang': 'en_US',\n",
        "            'X-Li-Page-Instance': 'urn:li:page:d_flagship3_search_srp_people_load_more;Ux/gXNk8TtujmdQaaFmrPA==',\n",
        "            'X-Li-Track': '{\"clientVersion\":\"1.13.9792\",\"mpVersion\":\"1.13.9792\",\"osName\":\"web\",\"timezoneOffset\":6,\"timezone\":\"Asia/Dhaka\",\"deviceFormFactor\":\"DESKTOP\",\"mpName\":\"voyager-web\",\"displayDensity\":1.3125,\"displayWidth\":1920.1875,\"displayHeight\":1080.1875}',\n",
        "            'X-Restli-Protocol-Version': '2.0.0',\n",
        "        }\n",
        "        for page_no in range(0, 10, 10):\n",
        "            print(\"Checking facet: {}/990\".format(page_no))\n",
        "            link = \"https://www.linkedin.com/voyager/api/graphql?variables=(start:\" + str(page_no) + \",origin:COMPANY_PAGE_CANNED_SEARCH,query:(flagshipSearchIntent:SEARCH_SRP,queryParameters:List((key:currentCompany,value:List(\" + \\\n",
        "                companyID + \\\n",
        "                \")),(key:resultType,value:List(PEOPLE))),includeFiltersInResponse:false))&queryId=voyagerSearchDashClusters.e1f36c1a2618e5bb527c57bf0c7ebe9f\"\n",
        "\n",
        "            try:\n",
        "                resp = s.get(link, headers=headers).json()\n",
        "            except:\n",
        "                print(\"Failed to open {}\".format(link))\n",
        "                continue\n",
        "            results = resp.get('included')\n",
        "            for person_data in results:\n",
        "                if person_data.get('$type') == \"com.linkedin.voyager.dash.search.EntityResultViewModel\":\n",
        "                    person_name = person_data.get('title').get('text')\n",
        "                    profile_link = person_data.get('navigationUrl')\n",
        "\n",
        "                    # Check if primarySubtitle is None before accessing its text attribute\n",
        "                    primary_subtitle = person_data.get('primarySubtitle')\n",
        "                    designation = primary_subtitle.get('text') if primary_subtitle else None\n",
        "\n",
        "                    # Similarly, check if secondarySubtitle is None before accessing its text attribute\n",
        "                    secondary_subtitle = person_data.get('secondarySubtitle')\n",
        "                    person_location = secondary_subtitle.get('text') if secondary_subtitle else None\n",
        "\n",
        "                    print(\"Profile Link: {}\".format(profile_link))\n",
        "                    print(\"Name: {}\".format(person_name))\n",
        "                    print(\"Designation: {}\".format(designation))\n",
        "                    print(\"Location: {}\".format(person_location))\n",
        "                    print()\n",
        "\n",
        "                    dataset = [profile_link, person_name, designation, person_location]\n",
        "                    self.saveData(dataset, output_file_name)\n",
        "            print(\"Waiting for {} seconds\".format(pagination_delay))\n",
        "            sleep(pagination_delay)\n",
        "\n",
        "def google_search(query, num_results=10, lang='en'):\n",
        "    \"\"\"\n",
        "    Perform a Google search and return a list of URLs.\n",
        "    \"\"\"\n",
        "    search_results = search(query, num_results=num_results, lang=lang)\n",
        "    return search_results\n",
        "\n",
        "def get_linkedin_url(company_name):\n",
        "    \"\"\"\n",
        "    Search Google for the company's LinkedIn profile and scrape the URL of the first result.\n",
        "    \"\"\"\n",
        "    query = f\"{company_name} LinkedIn\"\n",
        "    search_results = google_search(query)\n",
        "\n",
        "    for url in search_results:\n",
        "        if \"linkedin.com/company\" in url:\n",
        "            return url\n",
        "\n",
        "    return None\n",
        "\n",
        "def scrape_linkedin_url(company_name):\n",
        "    \"\"\"\n",
        "    Scrape the URL of the company's LinkedIn profile.\n",
        "    \"\"\"\n",
        "    linkedin_url = get_linkedin_url(company_name)\n",
        "    if linkedin_url:\n",
        "        response = requests.get(linkedin_url)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        return linkedin_url\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "s = requests.Session()\n",
        "\n",
        "# Example usage:\n",
        "file_name = pd.read_csv('./linkedin_scraping.csv') # put file directory here\n",
        "company_names = file_name['Name'].values[191:327]\n",
        "\n",
        "for company_name in company_names:\n",
        "    company_link = scrape_linkedin_url(company_name)\n",
        "    if company_link:\n",
        "        print(f\"The LinkedIn URL for {company_name} is: {company_link}\")\n",
        "    else:\n",
        "        print(f\"No LinkedIn profile found for {company_name}.\")\n",
        "        continue\n",
        "\n",
        "    output_file_name = sanitize_filename(f\"{company_name}.csv\")  # output CSV excel file name\n",
        "    pagination_delay = 5  # delay in seconds before going to the next page\n",
        "    cookies = 'li_sugr=d58f1b94-64ef-406a-a881-13b7207f39e0; bcookie=\"v=2&a3745c43-8f27-4c0b-8867-4e8192a2738e\"; bscookie=\"v=1&20240106221724031e9c6e-1a34-4ce7-85b6-b0620aa6ca90AQHShBCFTf0Wtb7OaFI-A-cV1AAcYvaZ\"; JSESSIONID=\"ajax:4470063861007695657\"; timezone=America/Toronto; li_theme=light; li_theme_set=app; _guid=e666fbaf-73fb-42ed-a14c-1e27dac787cf; aam_uuid=33147409925437713830234114304413153587; _gcl_au=1.1.234055212.1704585726; liap=true; dfpfpt=bbe26affd46d4a94b72253d36c29c7a3; li_at=AQEDATZ9R1EEbu3jAAABjUjy9-YAAAGN3h4_sk0ADBtX8e4WxsjhwPPQsWASxe3vkvF6_4Lq0uK06Wx_6VHRCj9c3kGyxxSHcs9hN2FZer1k_deBSbkVhDUcQeRbOF10uaIyo7trutvRUWNL6RaMB7vZ; gpv_pn=www.linkedin.com%2Fcompany%2Fid-redacted%2Fadmin%2Fanalytics%2Ffollowers%2F; s_ips=830.1999969482422; s_tslv=1708397158247; s_tp=2970; lang=v=2&lang=en-us; fptctx2=taBcrIH61PuCVH7eNCyH0F58uBDuZFZOunQHZt3FuglNSBz0kNWLrWQTo0Un%252f%252bynhuoqWYITZl7djIDr35X8NvU7Ef%252bwPUfmSdl%252bZiH6zjqMrEvyDwrTrTrZu%252ffj82G5X0IVl%252bBbtsl5uc4iH58rQudbasEO8F3ETBjvEmU1md%252by1E%252bNhCq7LXsQfiIUWMpTh0YKfFIJ5X8Z5q0KM6JeNrBNMBpMwa7pLG%252fdAIOwLrESC35Pconqc2VarYoZpwfrqgjm%252fEgc7w%252fKsglHp8L8KGB2uY3DDqb3mtc%252fdFo7C0WNtoJLJGHl0BphZReYJa5OBFzThVqkMZVfMrQsTYsovPxJq60B25EGpW6XVmcM27g%253d; AMCVS_14215E3D5995C57C0A495C55%40AdobeOrg=1; AMCV_14215E3D5995C57C0A495C55%40AdobeOrg=-637568504%7CMCIDTS%7C19775%7CMCMID%7C33730514080517143640283887874633419512%7CMCAAMLH-1709230877%7C7%7CMCAAMB-1709230877%7CRKhpRz8krg2tLO6pguXWp5olkAcUniQYPHaMWWgdJ3xzPWQmdj0y%7CMCOPTOUT-1708633277s%7CNONE%7CvVersion%7C5.1.1%7CMCCIDH%7C2007872224; UserMatchHistory=AQKMHpYxjqzFugAAAY3SOAIMTKS-RHKaYk5f68mSeRluDHEeDs-UmxLpovJApjYxod7vphXqeDNeHxDl5AiXeH7FdVd9W12uJt8XdClCVa3nGUt22y-HNebEo3Az585N8XJSfG61eJoG3FabphQyzToxHHSHnghRN4fHInfMbWsv-PGDdub2vqNS4lZBem8jJ4jD6WiA8S_r0HZd3a29LcF5g-Yh24hETmUx8wXXXVamp-czl4s2blvgdIzeEBJuHl8v6fnyg6-caiPRDC7sK6SKq07-bMbJQbpQy10LF46SLTHsRICkpHJy7b1uw3AepPAl06o; AnalyticsSyncHistory=AQLLiC8X_379BgAAAY3SOAIMUu5MKztoITkH6U3Pv1SxqwKPUxF5RGMVO91Q-o4Q8xkNdnNlpPno51kfPC9-Uw; lms_ads=AQEpnPYWQEA4EwAAAY3SOAJc6DZB1IT4PNWH63tVr7yd7qxvzPU4rhajhVItUDaMAMF6WpPSV69wY1zl94zgALnI678DpDdR; lms_analytics=AQEpnPYWQEA4EwAAAY3SOAJc6DZB1IT4PNWH63tVr7yd7qxvzPU4rhajhVItUDaMAMF6WpPSV69wY1zl94zgALnI678DpDdR; lidc=\"b=VB21:s=V:r=V:a=V:p=V:g=5296:u=813:x=1:i=1708628902:t=1708632183:v=2:sig=AQECmq4zsEcAVmlZ939tDqMOIbSkqnTd\"'  # place cookie here\n",
        "\n",
        "    if __name__ == \"__main__\":\n",
        "        companyID = LinkedIn.getCompanyID(company_link)\n",
        "        if companyID is not None:\n",
        "            linkedin = LinkedIn()\n",
        "            linkedin.paginateResults(companyID, cookies, pagination_delay, output_file_name)\n",
        "        else:\n",
        "            continue\n"
      ],
      "metadata": {
        "id": "lgFdZTgHq5Wb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "e63710f4-61d1-48fb-d142-f78d5e8ae206"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "HTTPError",
          "evalue": "429 Client Error: Too Many Requests for url: https://www.google.com/sorry/index?continue=https://www.google.com/search%3Fq%3DMistral%252BVenture%252BPartners%252BInc.%252BLinkedIn%26num%3D12%26hl%3Den%26start%3D0&hl=en&q=EgQiW4KAGJ6Z364GIjAz9H2Mbl0kSQcSNG64WYe-UnkgSpqNkZ2dIGRsaTfxRpB-e0k6I3NGhEudD-fXc90yAXJaAUM",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-57566b674863>\u001b[0m in \u001b[0;36m<cell line: 159>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcompany_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcompany_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m     \u001b[0mcompany_link\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscrape_linkedin_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompany_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcompany_link\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"The LinkedIn URL for {company_name} is: {company_link}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-57566b674863>\u001b[0m in \u001b[0;36mscrape_linkedin_url\u001b[0;34m(company_name)\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0mScrape\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mURL\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcompany\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mLinkedIn\u001b[0m \u001b[0mprofile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \"\"\"\n\u001b[0;32m--> 145\u001b[0;31m     \u001b[0mlinkedin_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_linkedin_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompany_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlinkedin_url\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinkedin_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-57566b674863>\u001b[0m in \u001b[0;36mget_linkedin_url\u001b[0;34m(company_name)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0msearch_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgoogle_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0murl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msearch_results\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"linkedin.com/company\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/googlesearch/__init__.py\u001b[0m in \u001b[0;36msearch\u001b[0;34m(term, num_results, lang, proxy, advanced, sleep_interval, timeout)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mnum_results\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;31m# Send request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         resp = _req(escaped_term, num_results - start,\n\u001b[0m\u001b[1;32m     55\u001b[0m                     lang, start, proxies, timeout)\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/googlesearch/__init__.py\u001b[0m in \u001b[0;36m_req\u001b[0;34m(term, results, lang, start, proxies, timeout)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     )\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1019\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: 429 Client Error: Too Many Requests for url: https://www.google.com/sorry/index?continue=https://www.google.com/search%3Fq%3DMistral%252BVenture%252BPartners%252BInc.%252BLinkedIn%26num%3D12%26hl%3Den%26start%3D0&hl=en&q=EgQiW4KAGJ6Z364GIjAz9H2Mbl0kSQcSNG64WYe-UnkgSpqNkZ2dIGRsaTfxRpB-e0k6I3NGhEudD-fXc90yAXJaAUM"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Get the list of CSV files in the directory\n",
        "csv_files = [file for file in os.listdir('/content') if file.endswith('.csv') and file != 'linkedin_scraping.csv']\n",
        "\n",
        "# Initialize an empty list to store DataFrames\n",
        "dfs = []\n",
        "\n",
        "# Read each CSV file and append its DataFrame to the list\n",
        "for file in csv_files:\n",
        "    df = pd.read_csv(os.path.join('/content', file))\n",
        "    df['Category'] = file[:-4]  # Add a category column based on the filename\n",
        "    dfs.append(df)\n",
        "\n",
        "# Concatenate all DataFrames into a single DataFrame\n",
        "merged_df = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "# Display the merged DataFrame\n",
        "print(merged_df)\n",
        "\n",
        "# Optionally, you can save the merged DataFrame to a new CSV file\n",
        "merged_df.to_csv('/content/merged_data.csv', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9d3lqFcA9fm_",
        "outputId": "1ffa9dd3-0780-4614-9694-d63c2df2605b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                           Profile Link  \\\n",
            "0     https://www.linkedin.com/in/krishna-shah07?min...   \n",
            "1     https://www.linkedin.com/in/annie-zou-50764b17...   \n",
            "2     https://www.linkedin.com/in/daniel-dampf-362b4...   \n",
            "3     https://www.linkedin.com/in/malency-wainwright...   \n",
            "4     https://www.linkedin.com/in/shenna-marie-bongg...   \n",
            "...                                                 ...   \n",
            "1544  https://www.linkedin.com/in/simranassi95?miniP...   \n",
            "1545  https://www.linkedin.com/in/wendy-lockwood-7b9...   \n",
            "1546  https://www.linkedin.com/in/apetroucic?miniPro...   \n",
            "1547  https://www.linkedin.com/in/tania-bickerstaffe...   \n",
            "1548  https://www.linkedin.com/in/mike-mannix?miniPr...   \n",
            "\n",
            "                                   Name  \\\n",
            "0                          Krishna Shah   \n",
            "1                             Annie Zou   \n",
            "2                          Daniel Dampf   \n",
            "3     Malency Wainwright, MBA, CPA, CMA   \n",
            "4                  Shenna Marie Bonggot   \n",
            "...                                 ...   \n",
            "1544                        Simran Assi   \n",
            "1545                     Wendy Lockwood   \n",
            "1546                    Andre Petroucic   \n",
            "1547                 Tania Bickerstaffe   \n",
            "1548            Mike Mannix, CFA, ICD.D   \n",
            "\n",
            "                                            Designation  \\\n",
            "0     Lead Business Intelligence Developer | Data An...   \n",
            "1     Data analyst | Front end developer | Python | ...   \n",
            "2     Co-Founder and CTO at Subtraid Inc, CTO at MCS...   \n",
            "3                                   Founder & President   \n",
            "4                              Working at MCS Analytics   \n",
            "...                                                 ...   \n",
            "1544                                 Compliance Analyst   \n",
            "1545              Vice President at Caldwell Securities   \n",
            "1546  Real Estate Developer, Angel Investor, Board M...   \n",
            "1547  Community Relations Liaison & Executive Assistant   \n",
            "1548  ...to get things done, you must love the doing...   \n",
            "\n",
            "                          Location                   Category  \n",
            "0                      Toronto, ON  MCS Analytics Corporation  \n",
            "1                           Canada  MCS Analytics Corporation  \n",
            "2                           Canada  MCS Analytics Corporation  \n",
            "3     Greater Toronto Area, Canada  MCS Analytics Corporation  \n",
            "4                         Paracale  MCS Analytics Corporation  \n",
            "...                            ...                        ...  \n",
            "1544                   Toronto, ON   Caldwell Securities Ltd.  \n",
            "1545  Greater Toronto Area, Canada   Caldwell Securities Ltd.  \n",
            "1546             São Paulo, Brazil       CIC Capital Ventures  \n",
            "1547                   Burnaby, BC      Band Capital Partners  \n",
            "1548                   Calgary, AB      Band Capital Partners  \n",
            "\n",
            "[1549 rows x 5 columns]\n"
          ]
        }
      ]
    }
  ]
}